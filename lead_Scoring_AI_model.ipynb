{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMqBHC170HZ/3eRILCCkmw5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WGHuW2h5isif","executionInfo":{"status":"ok","timestamp":1742359629323,"user_tz":-330,"elapsed":1133,"user":{"displayName":"Neelesh Gadi","userId":"07418898534377701938"}},"outputId":"f816cf2e-a783-47c4-bb38-56e4165c8330"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","✅ Accuracy: 100.00%\n","✅ ROC-AUC Score: 1.00\n","\n","✅ Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        67\n","           1       1.00      1.00      1.00        33\n","\n","    accuracy                           1.00       100\n","   macro avg       1.00      1.00      1.00       100\n","weighted avg       1.00      1.00      1.00       100\n","\n","\n","🔍 LEAD SCORING RESULTS:\n","User ID  Lead_Score Lead_Quality\n","    127    0.000000          Low\n","     59   17.659649          Low\n","     91   59.931364       Medium\n","    430   73.927262         High\n","    446    0.000000          Low\n","     24   53.934530       Medium\n","    121    0.000000          Low\n","    169    3.923319          Low\n","    124    0.000000          Low\n","    329   50.415906       Medium\n","     34    0.000000          Low\n","    429    0.000000          Low\n","     88   15.516322          Low\n","    173  100.000000         High\n","    286  100.000000         High\n","      1  100.000000         High\n","    338   91.975533         High\n","    190    0.000000          Low\n","    208  100.000000         High\n","     55   83.254801         High\n","     97    0.000000          Low\n","    175    0.000000          Low\n","     72    0.000000          Low\n","     84   53.379784       Medium\n","    458   26.771021          Low\n","    180    0.000000          Low\n","    248    0.000000          Low\n","    280   38.297182          Low\n","    360   57.767639       Medium\n","    125  100.000000         High\n","    374    0.000000          Low\n","    402    0.000000          Low\n","    362  100.000000         High\n","    377   31.564114          Low\n","    372   83.446360         High\n","    190  100.000000         High\n","    198    0.000000          Low\n","     83  100.000000         High\n","     47    0.000000          Low\n","    311    0.000000          Low\n","    289    0.000000          Low\n","     49   90.775711         High\n","    369    0.000000          Low\n","    472    0.000000          Low\n","    339    0.000000          Low\n","    143    0.000000          Low\n","      5   80.781852         High\n","    129   44.166327       Medium\n","    310   91.207199         High\n","    247  100.000000         High\n","    437  100.000000         High\n","    192    0.000000          Low\n","    419  100.000000         High\n","    447    0.000000          Low\n","    427    0.000000          Low\n","    285  100.000000         High\n","    212  100.000000         High\n","    342  100.000000         High\n","    486   20.632473          Low\n","    395   14.412728          Low\n","    163   34.208024          Low\n","     32  100.000000         High\n","    242  100.000000         High\n","     60    0.000000          Low\n","     41    0.000000          Low\n","    376   43.845538       Medium\n","     42    0.000000          Low\n","    232   76.079308         High\n","    462  100.000000         High\n","    441   45.593833       Medium\n","     75   21.228567          Low\n","    203   28.036673          Low\n","    195    4.585461          Low\n","    209    0.000000          Low\n","    373   66.243057       Medium\n","    308  100.000000         High\n","     12  100.000000         High\n","    353   30.209034          Low\n","    448   23.997294          Low\n","     40  100.000000         High\n","    236    0.000000          Low\n","    428  100.000000         High\n","    468   65.389851       Medium\n","    108  100.000000         High\n","    397   67.598250       Medium\n","    454    0.000000          Low\n","    277  100.000000         High\n","    335   54.455468       Medium\n","     31   47.720517       Medium\n","    301   70.954192         High\n","    463   54.423412       Medium\n","    148    0.000000          Low\n","    323  100.000000         High\n","    193  100.000000         High\n","     56   84.899647         High\n","     57  100.000000         High\n","    188   69.281647       Medium\n","    204    0.000000          Low\n","    234    0.000000          Low\n","    185    0.000000          Low\n","\n","📌 TOP 5 IMPORTANT FEATURES:\n","              Feature  Importance    Weight\n","Cart Abandonment Rate    0.549455 54.945535\n","      Order Frequency    0.238341 23.834091\n","     Session Duration    0.122555 12.255508\n","          Total Spend    0.033757  3.375675\n","         Review Score    0.023619  2.361894\n","\n","✅ Lead scoring results saved to 'lead_scoring_results.csv'\n"]}],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n","import numpy as np\n","import joblib\n","\n","# ✅ Load the synthetic data\n","file_path = 'lead_scoring_synthetic.csv'  # Adjust path if needed\n","data = pd.read_csv(file_path)\n","\n","# ✅ Drop unnecessary columns\n","data_cleaned = data.copy()\n","\n","# ✅ Fill missing values using .loc (Future-proof for pandas 3.0)\n","for col in data_cleaned.columns:\n","    if data_cleaned[col].dtype == 'object':\n","        data_cleaned.loc[:, col] = data_cleaned[col].fillna('Unknown')\n","    else:\n","        data_cleaned.loc[:, col] = data_cleaned[col].fillna(data_cleaned[col].mean())\n","\n","# ✅ Encode categorical columns using Label Encoding\n","label_encoder = LabelEncoder()\n","for col in data_cleaned.columns:\n","    if data_cleaned[col].dtype == 'object':\n","        data_cleaned.loc[:, col] = label_encoder.fit_transform(data_cleaned[col])\n","\n","# ✅ Split data into features and target\n","X = data_cleaned.drop(['Converted', 'User ID'], axis=1)\n","y = data_cleaned['Converted']\n","user_ids = data_cleaned['User ID']  # Save User ID for reference\n","\n","# ✅ Scale numeric features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# ✅ Split into train and test sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test, user_ids_train, user_ids_test = train_test_split(\n","    X_scaled, y, user_ids, test_size=0.2, random_state=42\n",")\n","\n","# ✅ Train a RandomForestClassifier\n","model = RandomForestClassifier(n_estimators=100, random_state=42)\n","model.fit(X_train, y_train)\n","\n","# ✅ Predictions\n","y_pred = model.predict(X_test)\n","y_prob = model.predict_proba(X_test)[:, 1]\n","\n","# ✅ Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","roc_auc = roc_auc_score(y_test, y_prob)\n","report = classification_report(y_test, y_pred)\n","\n","print(f\"\\n✅ Accuracy: {accuracy * 100:.2f}%\")\n","print(f\"✅ ROC-AUC Score: {roc_auc:.2f}\")\n","print(\"\\n✅ Classification Report:\\n\", report)\n","\n","# ✅ Extract Feature Importance\n","feature_importance = model.feature_importances_\n","features = X.columns\n","\n","importance_df = pd.DataFrame({\n","    'Feature': features,\n","    'Importance': feature_importance\n","}).sort_values(by='Importance', ascending=False)\n","\n","# ✅ Normalize importance to get feature weights\n","importance_df['Weight'] = (importance_df['Importance'] / importance_df['Importance'].sum()) * 100\n","\n","# ✅ Define the Scoring Function\n","def calculate_score(probability, feature_values):\n","    base_score = probability * 100\n","    feature_contribution = np.dot(feature_values, importance_df['Weight'].values)\n","    final_score = min(100, max(0, base_score + feature_contribution))\n","    return final_score\n","\n","# ✅ Generate Scores for Test Set\n","test_probabilities = model.predict_proba(X_test)[:, 1]\n","test_features = X_test\n","scores = [calculate_score(prob, features) for prob, features in zip(test_probabilities, test_features)]\n","\n","# ✅ Attach Scores and User ID to Results\n","results = pd.DataFrame(X_test, columns=features)\n","results['User ID'] = user_ids_test.values\n","results['Probability'] = test_probabilities\n","results['Lead_Score'] = scores\n","\n","# ✅ Define Lead Quality\n","def lead_quality(score):\n","    if score >= 70:\n","        return 'High'\n","    elif score >= 40:\n","        return 'Medium'\n","    else:\n","        return 'Low'\n","\n","results['Lead_Quality'] = results['Lead_Score'].apply(lead_quality)\n","\n","# ✅ Print Results in Terminal (ALL RECORDS)\n","print(\"\\n🔍 LEAD SCORING RESULTS:\")\n","print(results[['User ID', 'Lead_Score', 'Lead_Quality']].to_string(index=False))\n","\n","# ✅ Print Top 5 Important Features\n","print(\"\\n📌 TOP 5 IMPORTANT FEATURES:\")\n","print(importance_df.head().to_string(index=False))\n","\n","# ✅ Save the Model and Encoders\n","joblib.dump(model, 'lead_scoring_model.pkl')\n","joblib.dump(scaler, 'scaler.pkl')\n","joblib.dump(label_encoder, 'label_encoder.pkl')\n","\n","# ✅ Save Full Results to CSV\n","results.to_csv('lead_scoring_results.csv', index=False)\n","\n","print(\"\\n✅ Lead scoring results saved to 'lead_scoring_results.csv'\")\n"]}]}